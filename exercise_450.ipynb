{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Cross-Entropy Loss for a Batch of Data\n",
    "\n",
    "You may want to compute the loss for a batch of input data.\n",
    "\n",
    "You could compute the loss for each piece of data, one by one.\n",
    "\n",
    "$\\begin{bmatrix}6 & 2 & 1.9\\end{bmatrix}$\n",
    "$\\begin{bmatrix}5 & 2 & 1.9\\end{bmatrix}$\n",
    "$\\begin{bmatrix}4 & 2 & 1.9\\end{bmatrix}$\n",
    "\n",
    "But it's more efficient to compute the loss in a batch at one shot.\n",
    "\n",
    "We show below how the softmax function is applied to the output values to turn them into probabilities, and how a measure of the incorrectness, the cross-entropy loss, is computed from them, over a batch of three data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 and c2: tensor([[6.0000, 3.9000],\n",
      "        [5.0000, 3.9000],\n",
      "        [4.0000, 3.9000]])\n",
      "softmax: tensor([[0.8909, 0.1091],\n",
      "        [0.7503, 0.2497],\n",
      "        [0.5250, 0.4750]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "data = torch.Tensor([[6, 2, 1.9],[5, 2, 1.9],[4, 2, 1.9]])\n",
    "\n",
    "weights = torch.Tensor([[1, 0],[0, 1],[0, 1]])\n",
    "\n",
    "result = torch.mm(data, weights)\n",
    "print(\"c1 and c2: \" + str(result))\n",
    "\n",
    "result = F.softmax(torch.autograd.Variable(result), dim=1)\n",
    "print(\"softmax: \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a batch of data is processed, we will need to specify a batch of correct identifications in the target vector. In batch mode, nll_loss will average the loss for each target. Try varying different combinations of target (e.g. [1,1,1] or [0,0,0]) and you should be able to confirm the answers by averaging the respective items in the result tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log(softmax): tensor([[-0.1155, -2.2155],\n",
      "        [-0.2873, -1.3873],\n",
      "        [-0.6444, -0.7444]])\n",
      "tensor(1.4491)\n",
      "Loss: 1.4490838050842285\n"
     ]
    }
   ],
   "source": [
    "result = torch.log(result)\n",
    "print(\"log(softmax): \" + str(result))\n",
    "\n",
    "# The correct categories are a vector\n",
    "target = torch.LongTensor([0,0,0])\n",
    "\n",
    "loss = F.nll_loss(result, torch.autograd.Variable(target))\n",
    "print(loss)\n",
    "print(\"Loss: \"+str(loss.data.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch also has a ready-made function to compute cross entropy directly from the neural network's outputs.\n",
    "Convince yourself that the loss calculated by either method is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.349083811044693\n"
     ]
    }
   ],
   "source": [
    "loss = F.cross_entropy(result, torch.autograd.Variable(target))\n",
    "print(\"Loss: \"+str(loss.data.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
